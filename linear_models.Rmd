---
title: "Linear Models and Regression"

---


## Introduction

We consider a pair of random variables $(X,Y)$, we seek understanding relationships between the two variables and predicting $Y$ from $X$

A regression is a relation of the kind:
$$Y=f(X)+\epsilon$$
The function $f$ is the unknown "regression" function.

$\epsilon$ is the random error, with $E(\epsilon)=0$

$X$ is a random variable.
## Examples of Linear Models
 1. One way Analysis of Variance
 2.- Simple Linear Regression
 3.- Polynomial Regression
 4.- Simple Analysis of Covariance
 5.- Multiple Linear Regression
 
##Parameter Estimation

It consist in estimate $f$ and also the standard deviation $\sigma$ of the error $\epsilon=Y-f(X)$.We consider $n$ data pairs: ($X_1,Y_1$),($X_2,Y_2$),...,($X_n,Y_n$)
$$Y_i=f(X_i)+\epsilon_i$$
 $\forall i=1,..,n$  $E(\epsilon_i)=0$ $Var(\epsilon_i=\sigma^2)$

## Vector and Matrix Representations
We have 
\begin{align}
    y &= \begin{bmatrix}
           y_{1} \\
           y_{2} \\
           \vdots \\
           y_{n}
         \end{bmatrix}
         \in \mathbb{R^n}
  \end{align}
  
  \begin{align}
    \epsilon &= \begin{bmatrix}
           \epsilon_{1} \\
           \epsilon_{2} \\
           \vdots \\
           \epsilon_{n}
         \end{bmatrix}
         \in \mathbb{R^n}
  \end{align}
  
We denote basis functions  $f_1,f_2,...,f_p$ that can be writen as:
$$f(x)=\displaystyle\sum_{j=1}^{p}\theta_jf_j(x)$$

The vector of parameters
\begin{align}
    \theta &= \begin{bmatrix}
           \theta_{1} \\
           \theta_{2} \\
           \vdots \\
           \theta_{p}
         \end{bmatrix}
         \in \mathbb{R^p}
  \end{align}
  
The Design Matrix:
D=\begin{bmatrix}
    f_1(X_1) & f_2(X_1) &  \dots  & f_p{X_1} \\
    f_1(X_2) & f_2(X_2) &  \dots  & f_p{X_2}\\
    \vdots & \vdots & \vdots & \ddots  \\
    f_1(X_n) & f_2(X_n) &  \dots  & f_p{X_n}
\end{bmatrix}

Therefore our model can be written as follows:
$$Y=D\Theta+\epsilon$$

## Estimation of $\Theta$

We assume that $D$ has full column rank, it means that $rank(D)=p\leq n$.
In other words it means that the columns of $D$ are li. i.e for $\eta\in\mathbb{R^p}-\{0\}$ $D\eta\neq 0$, that is $0<||D\eta||^2=\eta^2D^TD\eta$. Therefore $D^TD$ is positive definite


## Least Square Estimation

A vector $\hat{\theta}\in\mathbb{R^p}$ is called a LEast Square Estimator of $\theta$ when:
$$||Y-D\hat{\theta}||^2=\displaystyle{ min_{\eta\in\mathbb{R^p}}}||Y-D\eta||^2$$

An estimator of $\theta$ is given by:
$$\hat{\theta}=(D^TD)^{-1}D^TY$$

\includegraphics[width=0.5]{Slides.pdf}